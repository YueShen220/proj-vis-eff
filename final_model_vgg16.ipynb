{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMsgqhAUDFiYglw6SzftsfG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YueShen220/proj-vis-eff/blob/main/final_model_vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb6ibtfSkD14"
      },
      "source": [
        "# Using Tensorflow framework as backend\n",
        "import os\n",
        "import tables\n",
        "import warnings\n",
        "import cv2\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from keras import models, layers, optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "% matplotlib inline\n",
        "style.use('seaborn-whitegrid')\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S89M0vKflAvb"
      },
      "source": [
        "# Helper functions to process image data to NumPy arrays of size (224, 224, 3)\n",
        "# and with backsubtraction and binary thresholding\n",
        "\n",
        "def process_data(X_data, y_data, rgb):\n",
        "    X_data = np.array(X_data, dtype = 'float32')\n",
        "    if rgb:\n",
        "        pass\n",
        "    else:\n",
        "        X_data = np.stack((X_data,)*3, axis=-1)\n",
        "    X_data /= 255\n",
        "    y_data = np.array(y_data)\n",
        "    y_data = to_categorical(y_data)\n",
        "    return X_data, y_data\n",
        "\n",
        "def walk_file_tree(relative_path):\n",
        "    X_data = []\n",
        "    y_data = [] \n",
        "    for directory, subdirectories, files in os.walk(relative_path):\n",
        "        for file in files:\n",
        "            if not file.startswith('.'):\n",
        "                path = os.path.join(directory, file)\n",
        "                gesture_name = gestures_index[file[9:11]]\n",
        "                y_data.append(gesture_name)\n",
        "\n",
        "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "                img = cv2.flip(img, 1)\n",
        "                if img is not None:\n",
        "                  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                blur = cv2.GaussianBlur(gray, (41, 41), 0)  #tuple indicates blur value\n",
        "                ret, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                thresh = cv2.resize(thresh, (224, 224))\n",
        "                thresh = np.array(thresh)\n",
        "                X_data.append(thresh)\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    X_data, y_data = process_data(X_data, y_data, False)\n",
        "    return X_data, y_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VMIIcYKqz53"
      },
      "source": [
        "## Load and process Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POesKz714RDl",
        "outputId": "3b8672c3-7192-4981-81fa-163c80d010c9"
      },
      "source": [
        "# Mount on Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d-D6lmj8FPf",
        "outputId": "29dd9458-d731-4901-9018-e9ac657d20fd"
      },
      "source": [
        "# Unzip the compressed folder on drive\n",
        "!unzip /content/drive/MyDrive/archive.zip > /dev/null\n",
        "!ls /content/leapGestRecog"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00  01\t02  03\t04  05\t06  07\t08  09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_R2xJkRrHgN"
      },
      "source": [
        "# Gesture map\n",
        "gestures_index = {'01': 1,   #Palm\n",
        "                  '02': 2,   #L\n",
        "                  '03': 3,   #Fist\n",
        "                  '04': 4,   #Fist moved\n",
        "                  '05': 5,   #Thumb\n",
        "                  '06': 6,   #Index\n",
        "                  '07': 7,   #Okay\n",
        "                  '08': 8,   #Palm moved\n",
        "                  '09': 9,   #C\n",
        "                  '10': 10   #Down\n",
        "                }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IznL6vWuNiml"
      },
      "source": [
        "# Preprocess the image dataset\n",
        "root_dir = '/content/leapGestRecog'\n",
        "X_data, y_data = walk_file_tree(root_dir)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42FNq21mevnw"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "HWdBj0IrHbvP",
        "outputId": "8e4cfd3b-9cf8-4da5-9105-13defeadbbbf"
      },
      "source": [
        "# Test and visualize the data\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "plt.imshow(X_data[19999])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 224, 224, 3)\n",
            "(20000, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa646d903d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAvDiUHGWlf7"
      },
      "source": [
        "## Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQSXBv6ZWmnB"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state=12, stratify=y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJ9VQQeeqng"
      },
      "source": [
        "## Save and load the the X, y data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvKEEZfFenc-"
      },
      "source": [
        "hdf5_file = tables.open_file('train_test_split.h5', mode='w')\n",
        "hdf5_file.create_array(hdf5_file.root,'X_train', obj=X_train)\n",
        "hdf5_file.create_array(hdf5_file.root,'y_train', obj=y_train)\n",
        "hdf5_file.create_array(hdf5_file.root,'X_test', obj=X_test)\n",
        "hdf5_file.create_array(hdf5_file.root,'y_test', obj=y_test)\n",
        "hdf5_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOta7SsOezXa"
      },
      "source": [
        "hdf5_file = tables.open_file('train_test_split.h5', mode='r')\n",
        "X_train = np.array(hdf5_file.root.X_train)\n",
        "y_train = np.array(hdf5_file.root.y_train)\n",
        "X_test = np.array(hdf5_file.root.X_test)\n",
        "y_test = np.array(hdf5_file.root.y_test)\n",
        "hdf5_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZxAXSLZXTMa"
      },
      "source": [
        "## Build the VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1v6e7VwdZz7"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/VGG_model.h5'\n",
        "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
        "                               min_delta=0,\n",
        "                               patience=10,\n",
        "                               verbose=1,\n",
        "                               mode='auto',\n",
        "                               restore_best_weights=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AmtQ0DFa8fa",
        "outputId": "ab7e6b36-86a3-487c-cf80-3563daf6259e"
      },
      "source": [
        "# Load VGG16\n",
        "# Get back the convolutional part of a VGG network trained on ImageNet\n",
        "\n",
        "imageSize = 224\n",
        "model1 = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
        "optimizer1 = optimizers.Adam()\n",
        "\n",
        "base_model = model1  # Topless\n",
        "# Add top layer\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(128, activation='relu', name='fc3')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu', name='fc4')(x)\n",
        "\n",
        "predictions = Dense(11, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Train top layer\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 128)               3211392   \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fc4 (Dense)                  (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 17,968,075\n",
            "Trainable params: 3,253,387\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igTgAnpib8sD",
        "outputId": "36d9532b-6fc7-4b3c-a39f-34ab9cc681a0"
      },
      "source": [
        "# Model training\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)]\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
        "          callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "\n",
        "# The following is NOT NEEDED, will not increase the model performance\n",
        "\n",
        "# Data augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=45.,\n",
        "#     width_shift_range=0.3,\n",
        "#     height_shift_range=0.3,\n",
        "#     horizontal_flip=True)\n",
        "\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# Fits the model on batches with real-time data augmentation:\n",
        "# model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "#           steps_per_epoch=len(X_train)/32, epochs=150, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "250/250 [==============================] - 8089s 32s/step - loss: 1.1298 - accuracy: 0.6189 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 2/200\n",
            "250/250 [==============================] - 8024s 32s/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0090 - val_accuracy: 0.9977\n",
            "Epoch 3/200\n",
            "250/250 [==============================] - 8001s 32s/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.0038 - val_accuracy: 0.9990\n",
            "Epoch 4/200\n",
            "250/250 [==============================] - 8016s 32s/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.0026 - val_accuracy: 0.9991\n",
            "Epoch 5/200\n",
            "250/250 [==============================] - 8019s 32s/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "Epoch 6/200\n",
            "250/250 [==============================] - 7743s 31s/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9923\n",
            "Epoch 7/200\n",
            "250/250 [==============================] - 8027s 32s/step - loss: 0.0289 - accuracy: 0.9938 - val_loss: 1.1738e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "250/250 [==============================] - 8055s 32s/step - loss: 2.2112e-04 - accuracy: 1.0000 - val_loss: 2.1103e-04 - val_accuracy: 0.9999\n",
            "Epoch 9/200\n",
            "250/250 [==============================] - 8068s 32s/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 9.2349e-04 - val_accuracy: 0.9996\n",
            "Epoch 10/200\n",
            "250/250 [==============================] - 8045s 32s/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 3.4972e-07 - val_accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "250/250 [==============================] - ETA: 0s - loss: 2.6360e-04 - accuracy: 1.0000 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMeE53B4Swk3"
      },
      "source": [
        "## Save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce0asgYDTFUJ"
      },
      "source": [
        "# Save the model to an hdf5 file\n",
        "model.save('VGG_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0CsgR5vTJ-q"
      },
      "source": [
        "# Save the model to a pickle file\n",
        "import pickle\n",
        "f = open('/content/drive/MyDrive/VGG16_model_only_weights.pkl', 'wb')\n",
        "pickle.dump(loaded_model.get_weights(), f)\n",
        "f.close()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Xc0R7hXocS"
      },
      "source": [
        "# Load the model from drive\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('/content/drive/MyDrive/VGG_model.h5')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1e6wK3xTcIZ"
      },
      "source": [
        "## Evaluate VGG16 cross-validated model with classification metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYpx7dETcXn"
      },
      "source": [
        "def get_classification_metrics(X_test, y_test):\n",
        "    pred = loaded_model.predict(X_test)\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    print(confusion_matrix(y_true, pred))\n",
        "    print('\\n')\n",
        "    print(classification_report(y_true, pred))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6QkslcTfVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ffd81f-97a7-4097-dd08-fb7d9b9c6f48"
      },
      "source": [
        "get_classification_metrics(X_data, y_data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2000    0    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0    0 2000    0    0    0    0    0    0    0]\n",
            " [   0    0    0 2000    0    0    0    0    0    0]\n",
            " [   0    0    0    0 2000    0    0    0    0    0]\n",
            " [   0    0    0    0    0 2000    0    0    0    0]\n",
            " [   0    0    0    0    0    0 2000    0    0    0]\n",
            " [   0    0    0    0    0    0    0 2000    0    0]\n",
            " [   0    0    0    0    0    0    0    0 2000    0]\n",
            " [   0    0    0    0    1    0    0    0    0 1999]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      2000\n",
            "           2       1.00      1.00      1.00      2000\n",
            "           3       1.00      1.00      1.00      2000\n",
            "           4       1.00      1.00      1.00      2000\n",
            "           5       1.00      1.00      1.00      2000\n",
            "           6       1.00      1.00      1.00      2000\n",
            "           7       1.00      1.00      1.00      2000\n",
            "           8       1.00      1.00      1.00      2000\n",
            "           9       1.00      1.00      1.00      2000\n",
            "          10       1.00      1.00      1.00      2000\n",
            "\n",
            "    accuracy                           1.00     20000\n",
            "   macro avg       1.00      1.00      1.00     20000\n",
            "weighted avg       1.00      1.00      1.00     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5NmSwbl2Ygu"
      },
      "source": [
        "## Predict gesture on a single RGB image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSQ3hKwh2bEM"
      },
      "source": [
        "gesture_names = {1: 'Palm',\n",
        "                 2: 'L',\n",
        "                 3: 'Fist',\n",
        "                 4: 'Fist moved',\n",
        "                 5: 'Thumb',\n",
        "                 6: 'Index',\n",
        "                 7: 'Okay',\n",
        "                 8: 'Palm moved',\n",
        "                 9: 'C',\n",
        "                 10: 'Down'}\n",
        "\n",
        "def predict_rgb_image_vgg(image):\n",
        "    image = np.array(image, dtype='float32')\n",
        "    image /= 255\n",
        "    pred_array = loaded_model.predict(image)\n",
        "    print(f'pred_array: {pred_array}')\n",
        "    result = gesture_names[np.argmax(pred_array)]\n",
        "    print(f'Result: {result}')\n",
        "    print(max(pred_array[0]))\n",
        "    score = float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
        "    print(result)\n",
        "    return result, score\n",
        "\n",
        "def predict_rgb_image(path):\n",
        "\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.flip(img, 1)\n",
        "    if img is not None:\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (41, 41), 0)  #tuple indicates blur value\n",
        "    ret, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    target = np.stack((thresh,) * 3, axis=-1)\n",
        "    target = cv2.resize(target, (224, 224))\n",
        "    target = target.reshape(1, 224, 224, 3)\n",
        "    prediction, score = predict_rgb_image_vgg(target)\n",
        "\n",
        "    print(f'Result: {prediction}, Score: {score}')\n",
        "    return prediction, score"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbNlfY0F2fTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacc80ce-5d0d-4e9d-fbc4-c2795a7037a4"
      },
      "source": [
        "predict_rgb_image('/content/drive/MyDrive/test2.jpg')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred_array: [[8.0924985e-17 1.0903999e-14 5.1684968e-15 3.5544319e-13 2.5051532e-12\n",
            "  6.1026024e-16 5.6379221e-14 1.0000000e+00 9.9604333e-18 1.7662450e-14\n",
            "  9.2193226e-16]]\n",
            "Result: Okay\n",
            "1.0\n",
            "Okay\n",
            "Result: Okay, Score: 100.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Okay', 100.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}