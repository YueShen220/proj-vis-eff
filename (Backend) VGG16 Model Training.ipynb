{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOljov1dec9WwdB4rSYYErB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YueShen220/proj-vis-eff/blob/main/(Backend)%20VGG16%20Model%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb6ibtfSkD14"
      },
      "source": [
        "# Using Tensorflow framework as backend\n",
        "import os\n",
        "import tables\n",
        "import warnings\n",
        "import cv2\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from keras import models, layers, optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "% matplotlib inline\n",
        "style.use('seaborn-whitegrid')\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S89M0vKflAvb"
      },
      "source": [
        "# Helper functions to process image data to NumPy arrays of size (224, 224, 3)\n",
        "# and with backsubtraction and binary thresholding\n",
        "\n",
        "def process_data(X_data, y_data, rgb):\n",
        "    X_data = np.array(X_data, dtype = 'float32')\n",
        "    if rgb:\n",
        "        pass\n",
        "    else:\n",
        "        X_data = np.stack((X_data,)*3, axis=-1)\n",
        "    X_data /= 255\n",
        "    y_data = np.array(y_data)\n",
        "    y_data = to_categorical(y_data)\n",
        "    return X_data, y_data\n",
        "\n",
        "def walk_file_tree(relative_path):\n",
        "    X_data = []\n",
        "    y_data = [] \n",
        "    for directory, subdirectories, files in os.walk(relative_path):\n",
        "        for file in files:\n",
        "            if not file.startswith('.'):\n",
        "                path = os.path.join(directory, file)\n",
        "                gesture_name = gestures_index[file[9:11]]\n",
        "                y_data.append(gesture_name)\n",
        "\n",
        "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "                img = cv2.flip(img, 1)\n",
        "                if img is not None:\n",
        "                  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                blur = cv2.GaussianBlur(gray, (41, 41), 0)  #tuple indicates blur value\n",
        "                ret, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                thresh = cv2.resize(thresh, (224, 224))\n",
        "                thresh = np.array(thresh)\n",
        "                X_data.append(thresh)\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    X_data, y_data = process_data(X_data, y_data, False)\n",
        "    return X_data, y_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VMIIcYKqz53"
      },
      "source": [
        "## Load and process Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POesKz714RDl",
        "outputId": "063fe585-93d8-4be5-a661-314e84cdb20c"
      },
      "source": [
        "# Mount on Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d-D6lmj8FPf",
        "outputId": "ed1ea5ee-93c3-49b6-9bf8-031be7cd11fb"
      },
      "source": [
        "# Unzip the compressed folder on drive\n",
        "!unzip /content/drive/MyDrive/archive.zip > /dev/null\n",
        "!ls /content/leapGestRecog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00  01\t02  03\t04  05\t06  07\t08  09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_R2xJkRrHgN"
      },
      "source": [
        "# Gesture map\n",
        "gestures_index = {'01': 1,   #Palm\n",
        "                  '02': 2,   #L\n",
        "                  '03': 3,   #Fist\n",
        "                  '04': 4,   #Fist moved\n",
        "                  '05': 5,   #Thumb\n",
        "                  '06': 6,   #Index\n",
        "                  '07': 7,   #Okay\n",
        "                  '08': 8,   #Palm moved\n",
        "                  '09': 9,   #C\n",
        "                  '10': 10   #Down\n",
        "                }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IznL6vWuNiml"
      },
      "source": [
        "# Preprocess the image dataset\n",
        "root_dir = '/content/leapGestRecog'\n",
        "X_data, y_data = walk_file_tree(root_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42FNq21mevnw"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "HWdBj0IrHbvP",
        "outputId": "ece430e1-7529-41c9-b29c-79a746f13f34"
      },
      "source": [
        "# Test and visualize the data\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "plt.imshow(X_data[19999])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-016a7ebc0c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test and visualize the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAvDiUHGWlf7"
      },
      "source": [
        "## Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQSXBv6ZWmnB"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state=12, stratify=y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJ9VQQeeqng"
      },
      "source": [
        "## Save and load the the X, y data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvKEEZfFenc-"
      },
      "source": [
        "hdf5_file = tables.open_file('train_test_split.h5', mode='w')\n",
        "hdf5_file.create_array(hdf5_file.root,'X_train', obj=X_train)\n",
        "hdf5_file.create_array(hdf5_file.root,'y_train', obj=y_train)\n",
        "hdf5_file.create_array(hdf5_file.root,'X_test', obj=X_test)\n",
        "hdf5_file.create_array(hdf5_file.root,'y_test', obj=y_test)\n",
        "hdf5_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOta7SsOezXa"
      },
      "source": [
        "hdf5_file = tables.open_file('train_test_split.h5', mode='r')\n",
        "X_train = np.array(hdf5_file.root.X_train)\n",
        "y_train = np.array(hdf5_file.root.y_train)\n",
        "X_test = np.array(hdf5_file.root.X_test)\n",
        "y_test = np.array(hdf5_file.root.y_test)\n",
        "hdf5_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZxAXSLZXTMa"
      },
      "source": [
        "## Build the VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1v6e7VwdZz7"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/VGG_model.h5'\n",
        "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
        "                               min_delta=0,\n",
        "                               patience=10,\n",
        "                               verbose=1,\n",
        "                               mode='auto',\n",
        "                               restore_best_weights=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AmtQ0DFa8fa",
        "outputId": "1425f757-7c57-431c-9eb0-79579f291ea9"
      },
      "source": [
        "# Load VGG16\n",
        "# Get back the convolutional part of a VGG network trained on ImageNet\n",
        "\n",
        "imageSize = 224\n",
        "model1 = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
        "optimizer1 = optimizers.Adam()\n",
        "\n",
        "base_model = model1  # Topless\n",
        "# Add top layer\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(128, activation='relu', name='fc3')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu', name='fc4')(x)\n",
        "\n",
        "predictions = Dense(11, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Train top layer\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 128)               3211392   \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "fc4 (Dense)                  (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 17,968,075\n",
            "Trainable params: 3,253,387\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igTgAnpib8sD",
        "outputId": "6405cb7e-3fda-4398-b439-2af5f3eeccdb"
      },
      "source": [
        "# Model training\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)]\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
        "          callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Save the model to an hdf5 file\n",
        "model.save('/models/VGG_model.h5')\n",
        "\n",
        "# Save the model to a pickle file\n",
        "import pickle\n",
        "pickle.dump(model, open('VGG_model.pkl','wb'))\n",
        "\n",
        "\n",
        "# The following is NOT NEEDED, will not increase the model performance\n",
        "\n",
        "# Data augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True,\n",
        "#     featurewise_std_normalization=True,\n",
        "#     rotation_range=45.,\n",
        "#     width_shift_range=0.3,\n",
        "#     height_shift_range=0.3,\n",
        "#     horizontal_flip=True)\n",
        "\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# Fits the model on batches with real-time data augmentation:\n",
        "# model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "#           steps_per_epoch=len(X_train)/32, epochs=150, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "250/250 [==============================] - 8089s 32s/step - loss: 1.1298 - accuracy: 0.6189 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 2/200\n",
            "250/250 [==============================] - 8024s 32s/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0090 - val_accuracy: 0.9977\n",
            "Epoch 3/200\n",
            "250/250 [==============================] - 8001s 32s/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.0038 - val_accuracy: 0.9990\n",
            "Epoch 4/200\n",
            "250/250 [==============================] - 8016s 32s/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.0026 - val_accuracy: 0.9991\n",
            "Epoch 5/200\n",
            "250/250 [==============================] - 8019s 32s/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
            "Epoch 6/200\n",
            "250/250 [==============================] - 7743s 31s/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0337 - val_accuracy: 0.9923\n",
            "Epoch 7/200\n",
            "188/250 [=====================>........] - ETA: 16:24 - loss: 0.0348 - accuracy: 0.9926"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMeE53B4Swk3"
      },
      "source": [
        "## Save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "ce0asgYDTFUJ",
        "outputId": "d43f73a3-cbbf-4961-d228-f9022018a44c"
      },
      "source": [
        "# Save the model to an hdf5 file\n",
        "model.save('/models/VGG_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-beada6936acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model to an hdf5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/models/VGG_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0CsgR5vTJ-q"
      },
      "source": [
        "# Save the model to a pickle file\n",
        "import pickle\n",
        "pickle.dump(model, open('VGG_model.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Xc0R7hXocS"
      },
      "source": [
        "from keras.models import load_model\n",
        "loaded_model = load_model('VGG_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1e6wK3xTcIZ"
      },
      "source": [
        "## Get classification metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYpx7dETcXn"
      },
      "source": [
        "def get_classification_metrics(X_test, y_test):\n",
        "    pred = model.predict(X_test)\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    print(confusion_matrix(y_true, pred))\n",
        "    print('\\n')\n",
        "    print(classification_report(y_true, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_f6hdJx2M8A"
      },
      "source": [
        "## VGG_cross_validated model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6QkslcTfVj"
      },
      "source": [
        "get_classification_metrics(X_data, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5NmSwbl2Ygu"
      },
      "source": [
        "## Predict gesture on a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSQ3hKwh2bEM"
      },
      "source": [
        "gesture_names = {1: 'Palm',\n",
        "                 2: 'L',\n",
        "                 3: 'Fist',\n",
        "                 4: 'Fist moved',\n",
        "                 5: 'Thumb',\n",
        "                 6: 'Index',\n",
        "                 7: 'Okay',\n",
        "                 8: 'Palm moved',\n",
        "                 9: 'C',\n",
        "                 10: 'Down'}\n",
        "\n",
        "def predict_rgb_image(path):\n",
        "    img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
        "    img2rgb = image_utils.img_to_array(img2rgb)\n",
        "    img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
        "    pred_array = model.predict(img2rgb)\n",
        "    result = gesture_names[np.argmax(pred_array)]\n",
        "    score =  float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
        "    print(f'Result: {result}, Score: {score}')\n",
        "    return result, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbNlfY0F2fTU"
      },
      "source": [
        "predict_rgb_image('/content/drive/MyDrive/test1.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}